{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Token Contribution Explainer\n",
    "\n",
    "This notebook provides a simple, interactive way to analyze why a language model makes a certain classification decision. It uses the `token_explainer.py` tool to visualize the contribution of each word, phrase, or subword in a given text.\n",
    "\n",
    "### How to Use:\n",
    "1.  **Install Dependencies:** Run the first cell to ensure all required packages are installed.\n",
    "2.  **Configure Inputs:** Modify the variables in the \"Settings\" cell below. You can change the text, the expected label, the prompt, and the masking strategy.\n",
    "3.  **Run All Cells:** Click `Run All` to execute the analysis from top to bottom.\n",
    "4.  **Analyze Results:**\n",
    "    *   The **DataFrame** will show the detailed metrics for each masked token, sorted by the most influential (`net_effect`).\n",
    "    *   The **Visual Report** at the bottom will display the text with highlights. Green tokens helped the model get closer to the correct answer, while red tokens pushed it toward an incorrect one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully imported token_explainer module from part2/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import the explainer module with better error handling\n",
    "try:\n",
    "    # Try direct import first\n",
    "    from token_explainer import (\n",
    "        get_logprobs_cached,\n",
    "        token_masking_analysis,\n",
    "        generate_html_report,\n",
    "        Tokenizer\n",
    "    )\n",
    "    print(\"✓ Successfully imported token_explainer module\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try importing from part2 subdirectory\n",
    "        from part2.token_explainer import (\n",
    "            get_logprobs_cached,\n",
    "            token_masking_analysis,\n",
    "            generate_html_report,\n",
    "            Tokenizer\n",
    "        )\n",
    "        print(\"✓ Successfully imported token_explainer module from part2/\")\n",
    "    except ImportError as e:\n",
    "        print(\"ERROR: Could not import token_explainer module.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(\"\\nPlease ensure that 'token_explainer.py' is in one of these locations:\")\n",
    "        print(\"  1. Same directory as this notebook\")\n",
    "        print(\"  2. In a 'part2' subdirectory\")\n",
    "        print(\"\\nCurrent working directory:\", Path.cwd())\n",
    "        raise\n",
    "\n",
    "# Widen column display for the DataFrame\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Settings\n",
    "Modify the variables in this cell to configure your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Settings configured\n",
      "  - Text length: 99 characters\n",
      "  - True label: 'pt'\n",
      "  - Strategy: 'word'\n",
      "  - Model: llama3:8b\n"
     ]
    }
   ],
   "source": [
    "# --- User Inputs ---\n",
    "text_to_analyze = \"os chefes de defesa da estónia, letónia, lituânia, alemanha, itália, espanha e eslováquia assinarão\"\n",
    "true_label = \"pt\"\n",
    "prompt_template = \"\"\"Given the text below, determine the single most appropriate ISO 639-1 language code.\n",
    "Answer with the code only.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Language code:\"\"\"\n",
    "\n",
    "# --- Strategy Settings ---\n",
    "# Options: 'word', 'phrase', or 'subword'\n",
    "strategy = 'word'\n",
    "phrase_size = 2  # Only used for 'phrase' strategy\n",
    "\n",
    "# --- Model Parameters ---\n",
    "logprob_kwargs = {\n",
    "    \"provider\": 'ollama',\n",
    "    \"model_id\": 'llama3:8b',\n",
    "    \"top_logprobs\": 5,\n",
    "    \"temperature\": 0.0,\n",
    "    \"invert_log\": False\n",
    "}\n",
    "\n",
    "print(\"✓ Settings configured\")\n",
    "print(f\"  - Text length: {len(text_to_analyze)} characters\")\n",
    "print(f\"  - True label: '{true_label}'\")\n",
    "print(f\"  - Strategy: '{strategy}'\")\n",
    "print(f\"  - Model: {logprob_kwargs['model_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Analysis\n",
    "This cell executes the main analysis logic based on the settings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Analysis\n",
      "============================================================\n",
      "\n",
      "[Step 1/4] Getting initial prediction...\n",
      "  ✓ Initial Prediction: 'pt'\n",
      "  ✓ True Label: 'pt'\n",
      "  ✓ Model prediction is CORRECT\n",
      "\n",
      "[Step 2/4] Preparing data...\n",
      "  ✓ Data prepared\n",
      "\n",
      "[Step 3/4] Tokenizer not needed for this strategy\n",
      "\n",
      "[Step 4/4] Running 'word' masking analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing with 'word' strategy: 100%|██████████| 1/1 [00:00<00:00, 65.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Analysis complete! Analyzed 14 tokens/phrases\n",
      "\n",
      "============================================================\n",
      "Analysis Complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Starting Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # a. Get initial prediction\n",
    "    print(\"\\n[Step 1/4] Getting initial prediction...\")\n",
    "    initial_prompt = prompt_template.format(text=text_to_analyze)\n",
    "    initial_res = get_logprobs_cached(prompt=initial_prompt, **logprob_kwargs)\n",
    "    unmasked_pred = initial_res.response_text.strip()\n",
    "    \n",
    "    print(f\"  ✓ Initial Prediction: '{unmasked_pred}'\")\n",
    "    print(f\"  ✓ True Label: '{true_label}'\")\n",
    "    \n",
    "    if unmasked_pred == true_label:\n",
    "        print(\"  ✓ Model prediction is CORRECT\")\n",
    "    else:\n",
    "        print(\"  ✗ Model prediction is INCORRECT\")\n",
    "    \n",
    "    # b. Prepare a DataFrame for the analysis function\n",
    "    print(\"\\n[Step 2/4] Preparing data...\")\n",
    "    input_df = pd.DataFrame([{\n",
    "        \"text\": text_to_analyze,\n",
    "        \"label\": true_label,\n",
    "        \"preds\": unmasked_pred\n",
    "    }])\n",
    "    print(\"  ✓ Data prepared\")\n",
    "    \n",
    "    # c. Load tokenizer if needed for 'subword' strategy\n",
    "    tokenizer = None\n",
    "    if strategy == 'subword':\n",
    "        print(\"\\n[Step 3/4] Loading 'subword' tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "            print(\"  ✓ Tokenizer loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ ERROR: Could not load tokenizer: {e}\")\n",
    "            print(\"  Falling back to 'word' strategy\")\n",
    "            strategy = 'word'\n",
    "            tokenizer = None\n",
    "    else:\n",
    "        print(\"\\n[Step 3/4] Tokenizer not needed for this strategy\")\n",
    "    \n",
    "    # d. Run the main masking analysis\n",
    "    print(f\"\\n[Step 4/4] Running '{strategy}' masking analysis...\")\n",
    "    if strategy == 'phrase':\n",
    "        print(f\"  - Phrase size: {phrase_size}\")\n",
    "    \n",
    "    analysis_df = token_masking_analysis(\n",
    "        df=input_df,\n",
    "        text_col='text',\n",
    "        label_col='label',\n",
    "        pred_col='preds',\n",
    "        prompt_template=prompt_template,\n",
    "        logprob_kwargs=logprob_kwargs,\n",
    "        strategy=strategy,\n",
    "        tokenizer=tokenizer,\n",
    "        phrase_size=phrase_size\n",
    "    )\n",
    "    \n",
    "    if analysis_df.empty:\n",
    "        print(\"  ✗ WARNING: Analysis returned no results\")\n",
    "    else:\n",
    "        print(f\"  ✓ Analysis complete! Analyzed {len(analysis_df)} tokens/phrases\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Analysis Complete\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR during analysis: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    analysis_df = pd.DataFrame()  # Create empty DataFrame for later cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results DataFrame\n",
    "\n",
    "The table below shows the detailed metrics for each token/phrase, sorted by `net_effect`. A high positive `net_effect` means that masking that token strongly helped the model get closer to the correct `true_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 14 tokens/phrases:\n",
      "\n",
      "Interpretation:\n",
      "  • Positive net_effect = token supports correct label\n",
      "  • Negative net_effect = token misleads the model\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>net_effect</th>\n",
       "      <th>support_label</th>\n",
       "      <th>suppress_competitor</th>\n",
       "      <th>entropy_change</th>\n",
       "      <th>top_choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da</td>\n",
       "      <td>4.342431</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>4.320588</td>\n",
       "      <td>-0.085192</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.146303</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chefes</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133245</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defesa</td>\n",
       "      <td>0.025892</td>\n",
       "      <td>0.025892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.105188</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>estónia,</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>os</td>\n",
       "      <td>-1.105919</td>\n",
       "      <td>-0.061222</td>\n",
       "      <td>-1.044697</td>\n",
       "      <td>0.183672</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>letónia,</td>\n",
       "      <td>-1.654240</td>\n",
       "      <td>-0.112124</td>\n",
       "      <td>-1.542116</td>\n",
       "      <td>0.284353</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eslováquia</td>\n",
       "      <td>-1.863350</td>\n",
       "      <td>-0.134743</td>\n",
       "      <td>-1.728607</td>\n",
       "      <td>0.311726</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e</td>\n",
       "      <td>-2.195141</td>\n",
       "      <td>-0.208837</td>\n",
       "      <td>-1.986304</td>\n",
       "      <td>0.434047</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>assinarão</td>\n",
       "      <td>-2.208427</td>\n",
       "      <td>-0.198319</td>\n",
       "      <td>-2.010108</td>\n",
       "      <td>0.406141</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lituânia,</td>\n",
       "      <td>-3.315424</td>\n",
       "      <td>-0.525423</td>\n",
       "      <td>-2.790001</td>\n",
       "      <td>0.598502</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>itália,</td>\n",
       "      <td>-3.617291</td>\n",
       "      <td>-0.667426</td>\n",
       "      <td>-2.949864</td>\n",
       "      <td>0.624425</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>espanha</td>\n",
       "      <td>-4.242277</td>\n",
       "      <td>-1.039128</td>\n",
       "      <td>-3.203149</td>\n",
       "      <td>0.633981</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alemanha,</td>\n",
       "      <td>-4.945685</td>\n",
       "      <td>-1.529529</td>\n",
       "      <td>-3.416156</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   masked_word  net_effect  support_label  suppress_competitor  \\\n",
       "4           da    4.342431       0.021843             4.320588   \n",
       "2           de    0.033732       0.033732                  NaN   \n",
       "1       chefes    0.031507       0.031507                  NaN   \n",
       "3       defesa    0.025892       0.025892                  NaN   \n",
       "5     estónia,    0.000875       0.000875                  NaN   \n",
       "0           os   -1.105919      -0.061222            -1.044697   \n",
       "6     letónia,   -1.654240      -0.112124            -1.542116   \n",
       "12  eslováquia   -1.863350      -0.134743            -1.728607   \n",
       "11           e   -2.195141      -0.208837            -1.986304   \n",
       "13   assinarão   -2.208427      -0.198319            -2.010108   \n",
       "7    lituânia,   -3.315424      -0.525423            -2.790001   \n",
       "9      itália,   -3.617291      -0.667426            -2.949864   \n",
       "10     espanha   -4.242277      -1.039128            -3.203149   \n",
       "8    alemanha,   -4.945685      -1.529529            -3.416156   \n",
       "\n",
       "    entropy_change top_choice  \n",
       "4        -0.085192         pt  \n",
       "2        -0.146303         pt  \n",
       "1        -0.133245         pt  \n",
       "3        -0.105188         pt  \n",
       "5         0.006581         pt  \n",
       "0         0.183672         pt  \n",
       "6         0.284353         pt  \n",
       "12        0.311726         pt  \n",
       "11        0.434047         pt  \n",
       "13        0.406141         pt  \n",
       "7         0.598502         pt  \n",
       "9         0.624425         et  \n",
       "10        0.633981         et  \n",
       "8         0.475931         pt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not analysis_df.empty:\n",
    "    # Define key columns to display for clarity\n",
    "    display_cols = [\n",
    "        'masked_word',\n",
    "        'net_effect',\n",
    "        'support_label',\n",
    "        'suppress_competitor',\n",
    "        'entropy_change',\n",
    "        'top_choice'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only columns that exist\n",
    "    available_cols = [col for col in display_cols if col in analysis_df.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(\"WARNING: Expected columns not found in results.\")\n",
    "        print(\"Available columns:\", list(analysis_df.columns))\n",
    "        display(analysis_df)\n",
    "    else:\n",
    "        # Sort by net_effect to see the most influential tokens first\n",
    "        if 'net_effect' in analysis_df.columns:\n",
    "            sorted_df = analysis_df.sort_values('net_effect', ascending=False)\n",
    "        else:\n",
    "            sorted_df = analysis_df\n",
    "        \n",
    "        print(f\"\\nShowing {len(sorted_df)} tokens/phrases:\")\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"  • Positive net_effect = token supports correct label\")\n",
    "        print(\"  • Negative net_effect = token misleads the model\")\n",
    "        print()\n",
    "        display(sorted_df[available_cols])\n",
    "else:\n",
    "    print(\"⚠ Analysis DataFrame is empty. Cannot display results.\")\n",
    "    print(\"Please check the analysis step above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual Report\n",
    "\n",
    "This visualization applies the results from the table above directly to the text.\n",
    "- **<span style='background-color: #c8ffc8;'>Green</span>** tokens contributed positively towards the correct label.\n",
    "- **<span style='background-color: #ffc8c8;'>Red</span>** tokens pushed the model towards an incorrect label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visual Token Contribution Report:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Token Contribution Report</title><style>body{font-family:sans-serif;line-height:1.6;margin:20px;background-color:#f4f4f4;color:#333;}.container{background-color:#fff;border:1px solid #e0e0e0;border-radius:8px;padding:15px;margin-bottom:20px;}h1,h3{color:#0056b3;}</style></head><body><h1>Token Contribution Analysis</h1>\n",
       "<div class='container'><h3>Prompt ID: 0</h3>\n",
       "<p><b>True Label:</b> <span style='color:green;'>pt</span> | <b>Prediction:</b> <span style='color:red;'>pt</span></p>\n",
       "<h4>Contribution Analysis (Strategy: word):</h4><div style='padding:10px;border:1px solid #ccc;border-radius:5px;'><span style=\"background-color: #fff2f2; border-radius: 3px;\">os</span> <span style=\"background-color: #fefffe; border-radius: 3px;\">chefes</span> <span style=\"background-color: #fefffe; border-radius: 3px;\">de</span> <span style=\"background-color: #fefffe; border-radius: 3px;\">defesa</span> <span style=\"background-color: #ceffce; border-radius: 3px;\">da</span> <span style=\"background-color: #fefffe; border-radius: 3px;\">estónia,</span> <span style=\"background-color: #ffecec; border-radius: 3px;\">letónia,</span> <span style=\"background-color: #ffdada; border-radius: 3px;\">lituânia,</span> <span style=\"background-color: #ffc8c8; border-radius: 3px;\">alemanha,</span> <span style=\"background-color: #ffd6d6; border-radius: 3px;\">itália,</span> <span style=\"background-color: #ffcfcf; border-radius: 3px;\">espanha</span> <span style=\"background-color: #ffe6e6; border-radius: 3px;\">e</span> <span style=\"background-color: #ffeaea; border-radius: 3px;\">eslováquia</span> <span style=\"background-color: #ffe6e6; border-radius: 3px;\">assinarão</span></div></div>\n",
       "</body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not analysis_df.empty:\n",
    "    try:\n",
    "        # Generate the HTML report using the results\n",
    "        html_report = generate_html_report(\n",
    "            analysis_df, \n",
    "            strategy=strategy, \n",
    "            phrase_size=phrase_size\n",
    "        )\n",
    "        \n",
    "        # Display the report directly in the notebook output\n",
    "        print(\"\\nVisual Token Contribution Report:\")\n",
    "        print(\"=\" * 60)\n",
    "        display(HTML(html_report))\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR generating visual report: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠ Analysis DataFrame is empty. Cannot display report.\")\n",
    "    print(\"Please check the analysis step above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics (Optional)\n",
    "Additional insights from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "============================================================\n",
      "Total tokens analyzed: 14\n",
      "  • Positive contributors (green): 5\n",
      "  • Negative contributors (red): 9\n",
      "  • Neutral: 0\n",
      "\n",
      "Net effect range: [-4.9457, 4.3424]\n",
      "Mean net effect: -1.4795\n",
      "\n",
      "Top 3 most helpful tokens:\n",
      "  • 'da': 4.3424\n",
      "  • 'de': 0.0337\n",
      "  • 'chefes': 0.0315\n",
      "\n",
      "Top 3 most misleading tokens:\n",
      "  • 'alemanha,': -4.9457\n",
      "  • 'espanha': -4.2423\n",
      "  • 'itália,': -3.6173\n"
     ]
    }
   ],
   "source": [
    "if not analysis_df.empty and 'net_effect' in analysis_df.columns:\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count positive vs negative contributors\n",
    "    positive = (analysis_df['net_effect'] > 0).sum()\n",
    "    negative = (analysis_df['net_effect'] < 0).sum()\n",
    "    neutral = (analysis_df['net_effect'] == 0).sum()\n",
    "    \n",
    "    print(f\"Total tokens analyzed: {len(analysis_df)}\")\n",
    "    print(f\"  • Positive contributors (green): {positive}\")\n",
    "    print(f\"  • Negative contributors (red): {negative}\")\n",
    "    print(f\"  • Neutral: {neutral}\")\n",
    "    \n",
    "    if 'net_effect' in analysis_df.columns:\n",
    "        print(f\"\\nNet effect range: [{analysis_df['net_effect'].min():.4f}, {analysis_df['net_effect'].max():.4f}]\")\n",
    "        print(f\"Mean net effect: {analysis_df['net_effect'].mean():.4f}\")\n",
    "        \n",
    "        # Show top contributors\n",
    "        print(\"\\nTop 3 most helpful tokens:\")\n",
    "        top_3 = analysis_df.nlargest(3, 'net_effect')[['masked_word', 'net_effect']]\n",
    "        for idx, row in top_3.iterrows():\n",
    "            print(f\"  • '{row['masked_word']}': {row['net_effect']:.4f}\")\n",
    "        \n",
    "        print(\"\\nTop 3 most misleading tokens:\")\n",
    "        bottom_3 = analysis_df.nsmallest(3, 'net_effect')[['masked_word', 'net_effect']]\n",
    "        for idx, row in bottom_3.iterrows():\n",
    "            print(f\"  • '{row['masked_word']}': {row['net_effect']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠ Cannot generate summary statistics - analysis data unavailable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
