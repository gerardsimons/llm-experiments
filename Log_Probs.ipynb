{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511dd8c5",
   "metadata": {},
   "source": [
    "## Log Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6d07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4195eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"dolphin-mistral\",\n",
    "    \"prompt\": \"Why is the sky blue?\",\n",
    "    \"stream\": False,\n",
    "}\n",
    "ret = requests.post(url, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1427ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"dolphin-mistral\",\"created_at\":\"2026-01-01T09:53:10.188414Z\",\"response\":\"The sky appears blue due to a combination of factors related to light and our atmosphere. When sunlight passes through Earth's atmosphere, it scatters in all directions. The shorter wavelengths, such as blue, are scattered more efficiently than the longer wavelengths, like red or yellow. This is because blue light has smaller particles that can scatter it more easily than other colors. As a result, we perceive the sky as blue because our eyes are most sensitive to blue light and our brains interpret this scattered light as a blue color.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[32001,1587,13,1976,460,15052,721,262,28725,264,10865,16107,13892,28723,13,32000,28705,13,32001,2188,13,7638,349,272,7212,5045,28804,32000,28705,13,32001,13892,13,1014,7212,8045,5045,2940,298,264,9470,302,8612,5202,298,2061,304,813,13789,28723,1684,22950,15167,1059,8599,28742,28713,13789,28725,378,752,270,1532,297,544,14278,28723,415,19367,275,26795,28713,28725,1259,390,5045,28725,460,20995,680,23463,821,272,3774,275,26795,28713,28725,737,2760,442,9684,28723,851,349,1096,5045,2061,659,7000,13938,369,541,11190,378,680,5061,821,799,9304,28723,1136,264,1204,28725,478,660,11642,272,7212,390,5045,1096,813,2282,460,1080,13509,298,5045,2061,304,813,26615,7190,456,20995,2061,390,264,5045,3181,28723],\"total_duration\":2010126875,\"load_duration\":36438250,\"prompt_eval_count\":34,\"prompt_eval_duration\":175391125,\"eval_count\":111,\"eval_duration\":1605813253}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(ret.content.decode('utf8'))\n",
    "resp = json.loads(ret.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [32001,\n",
      "             1587,\n",
      "             13,\n",
      "             1976,\n",
      "             460,\n",
      "             15052,\n",
      "             721,\n",
      "             262,\n",
      "             28725,\n",
      "             264,\n",
      "             10865,\n",
      "             16107,\n",
      "             13892,\n",
      "             28723,\n",
      "             13,\n",
      "             32000,\n",
      "             28705,\n",
      "             13,\n",
      "             32001,\n",
      "             2188,\n",
      "             13,\n",
      "             12069,\n",
      "             875,\n",
      "             1575,\n",
      "             272,\n",
      "             2296,\n",
      "             12271,\n",
      "             390,\n",
      "             2477,\n",
      "             6110,\n",
      "             442,\n",
      "             8250,\n",
      "             304,\n",
      "             865,\n",
      "             604,\n",
      "             6110,\n",
      "             442,\n",
      "             8250,\n",
      "             304,\n",
      "             2511,\n",
      "             1112,\n",
      "             28747,\n",
      "             330,\n",
      "             1384,\n",
      "             1207,\n",
      "             349,\n",
      "             272,\n",
      "             5565,\n",
      "             302,\n",
      "             1450,\n",
      "             12696,\n",
      "             32000,\n",
      "             28705,\n",
      "             13,\n",
      "             32001,\n",
      "             13892,\n",
      "             13,\n",
      "             6995],\n",
      " 'created_at': '2026-01-01T09:53:10.392213Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 2,\n",
      " 'eval_duration': 16434125,\n",
      " 'load_duration': 19254084,\n",
      " 'logprobs': [{'bytes': [70, 97, 108, 115, 101],\n",
      "               'logprob': -0.2837532162666321,\n",
      "               'token': 'False',\n",
      "               'top_logprobs': [{'bytes': [70, 97, 108, 115, 101],\n",
      "                                 'logprob': -0.2837532162666321,\n",
      "                                 'token': 'False'},\n",
      "                                {'bytes': [84, 114, 117, 101],\n",
      "                                 'logprob': -1.440993070602417,\n",
      "                                 'token': 'True'}]}],\n",
      " 'model': 'dolphin-mistral',\n",
      " 'prompt_eval_count': 58,\n",
      " 'prompt_eval_duration': 124952750,\n",
      " 'response': 'False',\n",
      " 'total_duration': 171891042}\n"
     ]
    }
   ],
   "source": [
    "# Check for classification results, not exactly as in training dataset\n",
    "\n",
    "def prompt_ollama(prompt_text, model=\"dolphin-mistral\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0,\n",
    "        \"logprobs\": True,\n",
    "        \"top_logprobs\": 2\n",
    "    }\n",
    "    return requests.post(url, json=payload).json()\n",
    "\n",
    "\n",
    "resp = prompt_ollama(\"Please classify the following sentence as either True or False and only return True or False and nothing else: Auckland is the capital of New Zealand\")\n",
    "pprint(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51194bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'False',\n",
       "  'logprob': -0.2837532162666321,\n",
       "  'bytes': [70, 97, 108, 115, 101],\n",
       "  'top_logprobs': [{'token': 'False',\n",
       "    'logprob': -0.2837532162666321,\n",
       "    'bytes': [70, 97, 108, 115, 101]},\n",
       "   {'token': 'True',\n",
       "    'logprob': -1.440993070602417,\n",
       "    'bytes': [84, 114, 117, 101]}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['logprobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a003b6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False -0.2837532162666321\n",
      "0.7529524381935939\n",
      "True -1.440993070602417\n",
      "0.2366925894792158\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for logp in resp['logprobs'][0]['top_logprobs']:\n",
    "    # print(logp, log[])\n",
    "    print(logp['token'], logp['logprob'])\n",
    "\n",
    "    p = math.e ** logp['logprob']\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0c2699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llama3:8b',\n",
       " 'nous-hermes:latest',\n",
       " 'dolphin-mistral:latest',\n",
       " 'deepseek-r1:14b']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://localhost:11434/api/tags\"\n",
    "payload = {\n",
    "    # \"model\": \"dolphin-mistral\",\n",
    "    # \"prompt\": \"Why is the sky blue?\",\n",
    "    # \"stream\": False\n",
    "}\n",
    "ret = requests.get(url, json=payload)\n",
    "model_ids = [x['name'] for x in ret.json()['models']]\n",
    "model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d012a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prompt_with_confidence_scores(prompt_text, model=\"dolphin-mistral\"):\n",
    "    resp = prompt_ollama(prompt_text=prompt_text, model=model)\n",
    "\n",
    "    result = {}\n",
    "    for logp in resp['logprobs'][0]['top_logprobs']:\n",
    "        # print(logp, log[])\n",
    "        # print(logp['token'], logp['logprob'])\n",
    "\n",
    "        p = math.e ** logp['logprob']\n",
    "        result[logp['token']] = p\n",
    "\n",
    "    return result\n",
    "\n",
    "prompt = \"Please answer the following question with True or False only. Amsterdam is the capital of Netherlands?\"\n",
    "# pred = classify_prompt_with_confidence_scores(prompt)\n",
    "# pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36921712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3:8b {'lazy': 0.5253462036219801, 'I': 0.147728060287421}\n",
      "nous-hermes:latest {' The': 0.44549545274199737, ' No': 0.03655876495276468}\n",
      "dolphin-mistral:latest {'l': 0.4884169001941493, 'The': 0.35448640103338597}\n",
      "deepseek-r1:14b {'<think>': 0.9999999852942977, '</think>': 2.385354525698507e-09}\n"
     ]
    }
   ],
   "source": [
    "def try_models(prompt, models=None):\n",
    "    if not models:\n",
    "        models = model_ids[:]\n",
    "    for m in model_ids:\n",
    "        pred = classify_prompt_with_confidence_scores(prompt, m)\n",
    "        print(m, pred)\n",
    "\n",
    "\n",
    "try_models(\"The quick brown fox jumped over the\", [\"llama3:8b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acd84921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [32001,\n",
      "             1587,\n",
      "             13,\n",
      "             1976,\n",
      "             460,\n",
      "             15052,\n",
      "             721,\n",
      "             262,\n",
      "             28725,\n",
      "             264,\n",
      "             10865,\n",
      "             16107,\n",
      "             13892,\n",
      "             28723,\n",
      "             13,\n",
      "             32000,\n",
      "             28705,\n",
      "             13,\n",
      "             32001,\n",
      "             2188,\n",
      "             13,\n",
      "             12069,\n",
      "             7446,\n",
      "             456,\n",
      "             12271,\n",
      "             297,\n",
      "             28705,\n",
      "             28770,\n",
      "             3085,\n",
      "             28725,\n",
      "             26167,\n",
      "             865,\n",
      "             395,\n",
      "             272,\n",
      "             6925,\n",
      "             3085,\n",
      "             28725,\n",
      "             459,\n",
      "             5683,\n",
      "             1077,\n",
      "             272,\n",
      "             2787,\n",
      "             28723,\n",
      "             4003,\n",
      "             349,\n",
      "             272,\n",
      "             12271,\n",
      "             28747,\n",
      "             415,\n",
      "             2936,\n",
      "             9060,\n",
      "             285,\n",
      "             1142,\n",
      "             14949,\n",
      "             754,\n",
      "             28705,\n",
      "             32000,\n",
      "             28705,\n",
      "             13,\n",
      "             32001,\n",
      "             13892,\n",
      "             13,\n",
      "             1014,\n",
      "             2936,\n",
      "             9060,\n",
      "             285,\n",
      "             1142,\n",
      "             14949,\n",
      "             754,\n",
      "             272,\n",
      "             17898,\n",
      "             3914,\n",
      "             28723],\n",
      " 'created_at': '2026-01-02T09:27:10.971999Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 12,\n",
      " 'eval_duration': 167283626,\n",
      " 'load_duration': 2357409500,\n",
      " 'logprobs': [{'bytes': [84, 104, 101],\n",
      "               'logprob': -0.6675326228141785,\n",
      "               'token': 'The',\n",
      "               'top_logprobs': [{'bytes': [84, 104, 101],\n",
      "                                 'logprob': -0.6675326228141785,\n",
      "                                 'token': 'The'},\n",
      "                                {'bytes': [116, 104, 101],\n",
      "                                 'logprob': -0.8558346629142761,\n",
      "                                 'token': 'the'}]},\n",
      "              {'bytes': [32, 113, 117, 105, 99, 107],\n",
      "               'logprob': -0.0097236642614007,\n",
      "               'token': ' quick',\n",
      "               'top_logprobs': [{'bytes': [32, 113, 117, 105, 99, 107],\n",
      "                                 'logprob': -0.0097236642614007,\n",
      "                                 'token': ' quick'},\n",
      "                                {'bytes': [32,\n",
      "                                           109,\n",
      "                                           105,\n",
      "                                           115,\n",
      "                                           115,\n",
      "                                           105,\n",
      "                                           110,\n",
      "                                           103],\n",
      "                                 'logprob': -6.1289167404174805,\n",
      "                                 'token': ' missing'}]},\n",
      "              {'bytes': [32, 98, 114, 111, 119, 110],\n",
      "               'logprob': -0.00043248283327557147,\n",
      "               'token': ' brown',\n",
      "               'top_logprobs': [{'bytes': [32, 98, 114, 111, 119, 110],\n",
      "                                 'logprob': -0.00043248283327557147,\n",
      "                                 'token': ' brown'},\n",
      "                                {'bytes': [44],\n",
      "                                 'logprob': -9.204184532165527,\n",
      "                                 'token': ','}]},\n",
      "              {'bytes': [32, 102],\n",
      "               'logprob': -0.0010172434849664569,\n",
      "               'token': ' f',\n",
      "               'top_logprobs': [{'bytes': [32, 102],\n",
      "                                 'logprob': -0.0010172434849664569,\n",
      "                                 'token': ' f'},\n",
      "                                {'bytes': [10],\n",
      "                                 'logprob': -9.156380653381348,\n",
      "                                 'token': '\\n'}]},\n",
      "              {'bytes': [111, 120],\n",
      "               'logprob': -7.078312046360224e-05,\n",
      "               'token': 'ox',\n",
      "               'top_logprobs': [{'bytes': [111, 120],\n",
      "                                 'logprob': -7.078312046360224e-05,\n",
      "                                 'token': 'ox'},\n",
      "                                {'bytes': [32, 111, 120],\n",
      "                                 'logprob': -10.49766731262207,\n",
      "                                 'token': ' ox'}]},\n",
      "              {'bytes': [32, 106, 117, 109, 112, 101, 100],\n",
      "               'logprob': -0.0019529322162270546,\n",
      "               'token': ' jumped',\n",
      "               'top_logprobs': [{'bytes': [32, 106, 117, 109, 112, 101, 100],\n",
      "                                 'logprob': -0.0019529322162270546,\n",
      "                                 'token': ' jumped'},\n",
      "                                {'bytes': [32, 106],\n",
      "                                 'logprob': -7.750553131103516,\n",
      "                                 'token': ' j'}]},\n",
      "              {'bytes': [32, 111, 118, 101, 114],\n",
      "               'logprob': -0.00033856541267596185,\n",
      "               'token': ' over',\n",
      "               'top_logprobs': [{'bytes': [32, 111, 118, 101, 114],\n",
      "                                 'logprob': -0.00033856541267596185,\n",
      "                                 'token': ' over'},\n",
      "                                {'bytes': [10],\n",
      "                                 'logprob': -9.409226417541504,\n",
      "                                 'token': '\\n'}]},\n",
      "              {'bytes': [32, 116, 104, 101],\n",
      "               'logprob': -0.07933580875396729,\n",
      "               'token': ' the',\n",
      "               'top_logprobs': [{'bytes': [32, 116, 104, 101],\n",
      "                                 'logprob': -0.07933580875396729,\n",
      "                                 'token': ' the'},\n",
      "                                {'bytes': [32],\n",
      "                                 'logprob': -3.726348400115967,\n",
      "                                 'token': ' '}]},\n",
      "              {'bytes': [32, 108, 97, 122, 121],\n",
      "               'logprob': -0.20313997566699982,\n",
      "               'token': ' lazy',\n",
      "               'top_logprobs': [{'bytes': [32, 108, 97, 122, 121],\n",
      "                                 'logprob': -0.20313997566699982,\n",
      "                                 'token': ' lazy'},\n",
      "                                {'bytes': [32],\n",
      "                                 'logprob': -3.0358235836029053,\n",
      "                                 'token': ' '}]},\n",
      "              {'bytes': [32, 100, 111, 103],\n",
      "               'logprob': -0.042998652905225754,\n",
      "               'token': ' dog',\n",
      "               'top_logprobs': [{'bytes': [32, 100, 111, 103],\n",
      "                                 'logprob': -0.042998652905225754,\n",
      "                                 'token': ' dog'},\n",
      "                                {'bytes': [32],\n",
      "                                 'logprob': -4.327545642852783,\n",
      "                                 'token': ' '}]},\n",
      "              {'bytes': [46],\n",
      "               'logprob': -0.02576512284576893,\n",
      "               'token': '.',\n",
      "               'top_logprobs': [{'bytes': [46],\n",
      "                                 'logprob': -0.02576512284576893,\n",
      "                                 'token': '.'},\n",
      "                                {'bytes': [60,\n",
      "                                           124,\n",
      "                                           105,\n",
      "                                           109,\n",
      "                                           95,\n",
      "                                           101,\n",
      "                                           110,\n",
      "                                           100,\n",
      "                                           124,\n",
      "                                           62],\n",
      "                                 'logprob': -4.535571575164795,\n",
      "                                 'token': '<|im_end|>'}]}],\n",
      " 'model': 'dolphin-mistral',\n",
      " 'prompt_eval_count': 63,\n",
      " 'prompt_eval_duration': 264324333,\n",
      " 'response': 'The quick brown fox jumped over the lazy dog.',\n",
      " 'total_duration': 2883682458}\n"
     ]
    }
   ],
   "source": [
    "resp = prompt_ollama(\"Please finish this sentence in 3 words, responding only with the missing words, not repeating the input. Here is the sentence: The quick brown fox jumped over \")\n",
    "pprint(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81adb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f3d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
